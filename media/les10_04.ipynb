{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b418998-b88a-4d0c-b040-5f04608f0f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10_04\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "\n",
    "#load images\n",
    "img1 = cv2.imread('right.png')\n",
    "img2 = cv2.imread('left.png')\n",
    "\n",
    "#creating a feature extraction\n",
    "orb = cv2.ORB_create()\n",
    "\n",
    "#compute the features in both images\n",
    "kpt1, desc1 = orb.detectAndCompute(img1, None) #img from which we need to extract the features\n",
    "kpt2, desc2 = orb.detectAndCompute(img2, None) #img from which we need to extract the features\n",
    "\n",
    "#create matcher and match key points\n",
    "matcher = cv2.BFMatcher(cv2.NORM_HAMMING)\n",
    "matches = matcher.knnMatch(desc1, desc2, k=2) #first element of the array matches with other point of the second img\n",
    "\n",
    "#How can we get rid of false positive\n",
    "#We can do it by ratio test, if we get second 2 closest pts and we compute ratio, if this ratio is below certain threshold then it is correct\n",
    "\n",
    "#how to perform the ratio test\n",
    "good_matches = []\n",
    "for m,n in matches: #m,n 2 elems in matches pt\n",
    "    if m.distance < 0.3 * n.distance: #0.8 because to be sure that we get enough pts then we ll lover it\n",
    "        good_matches.append(m)\n",
    "\n",
    "#We need to compute homography mtrx because in ideal case we have 2 imgs , they can easily steched together, but\n",
    "#in real life you introduce new transformation (tranlation, rotation) while shooting, there you cannot use Affine transformation, you need to \n",
    "#use prospective transformation. Previously we used to put 4 pts, now we do not need it bevcause we already extracting pts from img automatically we ll simply use the matches\n",
    "#We need to check if we have at loeast 4 pts for perspective transformation\n",
    "if len(good_matches) > 4: #then we can computre our homography mtrx\n",
    "    #convert pts to float 32 othwise opencv complains\n",
    "    src_points = np.float32([kpt1[m.queryIdx].pt for m in good_matches]) #in order to compute homo mtrx we do not need descriptor we just \n",
    "    #need pos pts. We are using good mtch inf=dex to get coordinate of the pt \".pt\" in the line above means point. kpt1 does contain other\n",
    "    #things on top of pts so we specify that we want to extract pts\n",
    "    #left img is querry img, right img is training img\n",
    "    #--------------------------------\n",
    "    dst_points = np.float32([kpt2[m.trainIdx].pt for m in good_matches]) \n",
    "    #2 sets of pts are called querryIdx, they belong to left img, and trainIdx is right one. Inside of good_matches we have matches(querryIdx, trainIdx)\n",
    "    #the position of good matches is stored img array\n",
    "    #querryIdx contains an index inside of left img, and trainIdx contains an index inside of right img\n",
    "    #So we will use only good matches, those that passed the ratio test\n",
    "\n",
    "    #Compute the homography matrix\n",
    "    M, mask = cv2.findHomography(src_points, dst_points)\n",
    "    #Now we can apply this set of transformations to see how src pts will fit to dest pts\n",
    "    #M will contain transformations, rotations\n",
    "    #transform the img and stitch it together with the right img\n",
    "    dst = cv2.warpPerspective(img2, M, (img1.shape[1] + img2.shape[1], img1.shape[0])) #why do we need provide the size cz while transformation it can become bigger so to contain all pts\n",
    "    dst[0:img2.shape[0], 0:img2.shape[1]] = img2.copy()\n",
    "\n",
    "cv2.namedWindow(\"Panorama\", cv2.WINDOW_KEEPRATIO)\n",
    "cv2.imshow(\"Panorama\", dst)\n",
    "cv2.waitKey(0)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb353cae-a75f-426e-8696-604de0d586ea",
   "metadata": {},
   "source": [
    "## Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a521b04-be6c-4560-9911-ed62287b9b89",
   "metadata": {},
   "source": [
    "-Text recognition\n",
    "-Speech recognition\n",
    "-Examples\n",
    "    -Alisa, Siri\n",
    "-History of NLP\n",
    "    -Symbolic NLP(1950 - 1990) \n",
    "\n",
    "Neural NLP -> NLP with neural network\n",
    "Reasons why NLP is harder then computer vision:\n",
    "-imgs contain numbers so with numbers you can do whatever u like\n",
    "But, nlp was created for humans by humans.\n",
    "-ambiguity, scale, sparsity, variation, expressivity\n",
    "\n",
    "Ambiguity -> any language have an ambiguity\n",
    "ambiguity can be in a different level (sentence is the same but meanings are different)\n",
    "Nlp does not scale linearly\n",
    "Proble  of sparcity\n",
    "there are most frequent words, and less frequent words\n",
    "Variation\n",
    "Expressivity -> same meaning can be expressed with different words, expressivity involves also emotions\n",
    "Sentiment analysis\n",
    "Analize text found on the web \n",
    "\n",
    "Why NLP improved?\n",
    "Mainly because of hardware power\n",
    "GPUs are the main component to train the model\n",
    "Raise of the web\n",
    "\n",
    "friday we will do nlpk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
